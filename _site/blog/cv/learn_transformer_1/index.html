<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Blog and Learning Notes</title>
  <meta name="viewport" content="width=device-width">
  <meta name="description" content="">
  <link rel="canonical" href="http://localhost:4001/blog/cv/learn_transformer_1/">

  <!-- Custom CSS -->
  <link rel="stylesheet" href="/blog/css/main.css">

  <!-- Google fonts -->
  <!-- <link href='https://fonts.googleapis.com/css?family=Roboto:400,300' rel='stylesheet' type='text/css'> -->

  <!-- Google tracking -->
  <!-- <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-46895817-2', 'auto');
    ga('send', 'pageview');
  </script> -->
</head>


    <body>

      <script src="https://unpkg.com/vanilla-back-to-top@7.2.1/dist/vanilla-back-to-top.min.js"></script>
      <script>addBackToTop({
        backgroundColor: '#fff',
        innerHTML: 'Back to Top',
        textColor: '#333'
      })</script>
      <style>
        #back-to-top {
          border: 1px solid #ccc;
          border-radius: 0;
          font-family: sans-serif;
          font-size: 14px;
          width: 100px;
          text-align: center;
          line-height: 30px;
          height: 30px;
        }
      </style>

    <header class="site-header">

  <a class="site-title" href="http://localhost:4001">Blog and Learning Notes</a>

</header>


    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1></h1>
  </header>

  <article class="post-content">
  <ul>
  <li><a href="#前言">前言</a></li>
  <li><a href="#注意力机制">注意力机制</a></li>
  <li><a href="#自注意力self-attention">自注意力（self-attention）</a></li>
  <li><a href="#多头自注意力multi-head-self-attention">多头自注意力（multi-head self-attention）</a></li>
  <li><a href="#总结">总结</a></li>
  <li><a href="#参考文献">参考文献</a></li>
</ul>

<h1 id="前言">前言</h1>
<p>自从Transformer[3]模型在NLP领域问世后，基于Transformer的深度学习模型性能逐渐在NLP和CV领域取得了令人惊叹的提升。本文的主要目的是结合CV视角来介绍Transformer模型的技术细节及基本原理，以方便读者在CV领域了解和使用Transformer。由于篇幅过长，本文将分为四个部分进行介绍，包括：
（1）自注意力和多头注意力模型的原理与实现。
（2）Transformer的整体架构与实现。
（3）位置编码（positional encoding）的原理与实现。
（4）Transformer在CV领域的应用案例。</p>

<p>本文首先讲解第一个话题：自注意力与多头注意力模型的原理与实现。</p>

<h1 id="注意力机制">注意力机制</h1>
<p>在CV领域，注意力机制并不陌生，如经典的spacial attention、channel attention、SE Block、CBAM等，其核心思想是自适应地提升特征表达在空间维度或（和）通道维度对特定位置的权重，增加神经网络对特定区域的关注程度。</p>

<p>在Transformer中，自注意力机制可以直观地理解为在学习一种关系，即一句话中当前字符与其他字符之间联系的紧密程度。</p>

<p>自注意力机制可以认为是Transformer的核心技术，掌握了自注意力和多头注意力模块，Transformer模型就不难理解了。</p>

<h1 id="自注意力self-attention">自注意力（self-attention）</h1>

<h1 id="多头自注意力multi-head-self-attention">多头自注意力（multi-head self-attention）</h1>
<h1 id="总结">总结</h1>
<p>我们发现，Transformer中的自注意力在大量的学习一种关系，即当前token和其他tokens之间的关联性，即使两个tokens之间的距离非常远，只要他们被考虑在内，就会计算它们之间的关联。（有点类似协方差矩阵中随机变量内部元素之间的相关性）</p>

<p>上述这种关系就是通过$QK^T$计算而来，二者的计算结果为一个权重系数，再将权重系数与对所有对应的$V$进行加权求和，即可得到当前token的新特征。</p>

<p>最后，我们将Transformer中多头注意力模型和CV中的Attention模型进行类比，可以发现二者之间有一些关联之处。令$Q=K=src+pos$, $V=src$，其中$src$为CNN backbone提取的高维特征，$pos$为图像的位置编码，则可以认为多头注意力模型就是在channel维度对特征进行了分组（每个头为一组），并对每个组别进行spacial attention，最后再合并计算最终输出。当然，需要注意，这个过程可能不是很严谨。</p>

<h1 id="参考文献">参考文献</h1>
<p>[1] <a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></p>

<p>[2] <a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a></p>

<p>[3] Vaswani et al. <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>, NIPS 2017.</p>

<p>[4] <a href="https://zhuanlan.zhihu.com/p/386579206">DETR详解</a></p>

  </article>

</div>

      </div>
    </div>

    <!-- <footer class="site-footer">

  <div class="wrap">

    <div class="footer-col-1 column">
      <ul>

        
        
        <!-- <li>
          <a href="mailto:"></a>
        </li> -->
       @xuyangcao
      </ul>
    </div>

    <div class="footer-col-2 column">

    </div>

    <div class="footer-col-3 column">

    </div>

  </div>

</footer>
 -->

    <!-- mathjax -->
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">
      // Make responsive
      MathJax.Hub.Config({
       "HTML-CSS": { linebreaks: { automatic: true } },
       "SVG": { linebreaks: { automatic: true } },
      });
    </script>

    </body>
</html>
